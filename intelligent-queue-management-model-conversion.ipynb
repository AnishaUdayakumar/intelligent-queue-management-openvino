{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e7d62b-5779-4211-822c-457c77321f8b",
   "metadata": {},
   "source": [
    "# Convert and Optimize YOLOv8 with OpenVINO™\n",
    "\n",
    "The YOLOv8 algorithm developed by Ultralytics is a cutting-edge, state-of-the-art (SOTA) model that is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection, image segmentation, and image classification tasks.\n",
    "\n",
    "YOLO stands for “You Only Look Once”, it is a popular family of real-time object detection algorithms. \n",
    "The original YOLO object detector was first released in 2016. Since then, different versions and variants of YOLO have been proposed, each providing a significant increase in performance and efficiency.\n",
    "YOLOv8 builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility.\n",
    "More details about its realization can be found in the original model [repository](https://github.com/ultralytics/ultralytics).\n",
    "\n",
    "Real-time object detection and instance segmentation are often used as key components in computer vision systems. \n",
    "Applications that use real-time object detection models include video analytics, robotics, autonomous vehicles, multi-object tracking and object counting, medical image analysis, and many others.\n",
    "\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to run and optimize PyTorch\\* YOLOv8 with OpenVINO. We consider the steps required for object detection and instance segmentation scenarios.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Prepare PyTorch model\n",
    "- Download and prepare a dataset\n",
    "- Validate the original model\n",
    "- Convert PyTorch model to OpenVINO IR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a12678-b12f-48d1-9735-398855733e46",
   "metadata": {},
   "source": [
    "## Get Pytorch model\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights.\n",
    "We will use the YOLOv8 medium model (also known as `yolov8m`) pre-trained on COCO dataset, which is available in this [repo](https://github.com/ultralytics/ultralytics). Similar steps are also applicable to other YOLOv8 models.\n",
    "Typical steps to obtain a pre-trained model:\n",
    "1. Create an instance of a model class\n",
    "2. Load checkpoint state dict, which contains pre-trained model weights\n",
    "3. Turn the model to evaluation for switching some operations to inference mode\n",
    "\n",
    "In this case, the model creators provide an API that enables converting the YOLOv8 model to ONNX and then to OpenVINO IR, so we don't need to do these steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2267760-cbfe-41c6-958d-cad9f845d5bb",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "\n",
    "Install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d04872-6916-454c-9211-6c644b50dc04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"ultralytics==8.0.43\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb6b88-0e19-4d5d-ab15-3d32a94800e4",
   "metadata": {},
   "source": [
    "Define utility functions for drawing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1d747-bb36-4a33-b7dc-86955e54dc8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics.yolo.utils.plotting import colors\n",
    "\n",
    "\n",
    "def plot_one_box(box:np.ndarray, img:np.ndarray, color:Tuple[int, int, int] = None, mask:np.ndarray = None, label:str = None, line_thickness:int = 5):\n",
    "    \"\"\"\n",
    "    Helper function for drawing single bounding box on image\n",
    "    Parameters:\n",
    "        x (np.ndarray): bounding box coordinates in format [x1, y1, x2, y2]\n",
    "        img (no.ndarray): input image\n",
    "        color (Tuple[int, int, int], *optional*, None): color in BGR format for drawing box, if not specified will be selected randomly\n",
    "        mask (np.ndarray, *optional*, None): instance segmentation mask polygon in format [N, 2], where N - number of points in contour, if not provided, only box will be drawn\n",
    "        label (str, *optonal*, None): box label string, if not provided will not be provided as drowing result\n",
    "        line_thickness (int, *optional*, 5): thickness for box drawing lines\n",
    "    \"\"\"\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    if mask is not None:\n",
    "        image_with_mask = img.copy()\n",
    "        mask\n",
    "        cv2.fillPoly(image_with_mask, pts=[mask.astype(int)], color=color)\n",
    "        img = cv2.addWeighted(img, 0.5, image_with_mask, 0.5, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_results(results:Dict, source_image:np.ndarray, label_map:Dict):\n",
    "    \"\"\"\n",
    "    Helper function for drawing bounding boxes on image\n",
    "    Parameters:\n",
    "        image_res (np.ndarray): detection predictions in format [x1, y1, x2, y2, score, label_id]\n",
    "        source_image (np.ndarray): input image for drawing\n",
    "        label_map; (Dict[int, str]): label_id to class name mapping\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    boxes = results[\"det\"]\n",
    "    masks = results.get(\"segment\")\n",
    "    h, w = source_image.shape[:2]\n",
    "    for idx, (*xyxy, conf, lbl) in enumerate(boxes):\n",
    "        label = f'{label_map[int(lbl)]} {conf:.2f}'\n",
    "        mask = masks[idx] if masks is not None else None\n",
    "        source_image = plot_one_box(xyxy, source_image, mask=mask, label=label, color=colors(int(lbl)), line_thickness=1)\n",
    "    return source_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373658bd-7e64-4479-914e-f2742d330afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"../data/datasets/person_detection/image_000009.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32fd08-650c-4751-bb41-d8afccb2495e",
   "metadata": {},
   "source": [
    "## Instantiate model\n",
    "\n",
    "There are several models available in the original repository, targeted for different tasks. For loading the model, required to specify a path to the model checkpoint. It can be some local path or name available on models hub (in this case model checkpoint will be downloaded automatically). \n",
    "\n",
    "Making prediction, the model accepts a path to input image and returns list with Results class object. Results contains boxes for object detection model and boxes and masks for segmentation model. Also it contains utilities for processing results e.g. `plot()` method for drawing.\n",
    "\n",
    "Let's consider examples:\n",
    "\n",
    "### Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdb05e-02a6-48f6-ac64-7199f0c331fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "DET_MODEL_NAME = \"yolov8m\"\n",
    "\n",
    "det_model = YOLO(f'{DET_MODEL_NAME}.pt')\n",
    "label_map = det_model.model.names\n",
    "\n",
    "res = det_model(IMAGE_PATH, save=True, classes=[0])\n",
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345ffcc-c4b8-44ba-8b03-f37e63a060da",
   "metadata": {},
   "source": [
    "### Convert model to OpenVINO IR\n",
    "\n",
    "YOLOv8 provides API for convenient model exporting to different formats including OpenVINO IR. `model.export` is responsible for model conversion. We need to specify the format and additionally, we can preserve dynamic shapes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2cc576-50c9-409f-be86-ad7122dce24b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    " \n",
    "# object detection model\n",
    "det_model_path = Path(f\"{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\")\n",
    "if not det_model_path.exists():\n",
    "    det_model.export(format=\"openvino\", dynamic=True, half=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713cd45f-2b19-4a1e-bc9d-5e69bd95d896",
   "metadata": {},
   "source": [
    "### Verify model inference\n",
    "\n",
    "To test model work, we create inference pipeline similar to `model.predict` method. Our pipeline consists of preprocessing step, inference of OpenVINO model and results post-processing to get results.\n",
    "The main difference in models for object detection and instance segmentation in postprocessing part, so input specification and preprocessing are common for both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f151e-0dfe-4005-bee6-49130b519854",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Model input is tensor with shape `[-1, 3, -1, -1]` in `N, C, H, W` format, where\n",
    "* `N` - number of images in batch (batch size)\n",
    "* `C` - image channels\n",
    "* `H` - image height\n",
    "* `W` - image width\n",
    "\n",
    "The model expects images in RGB channels format and normalized in [0, 1] range. Although the model supports dynamic input shape with preserving input divisibility to 32, it is recommended to use static shapes e.g. 640x640 for better efficiency. To resize images to fit model size `letterbox` resize approach is used where the aspect ratio of width and height is preserved.\n",
    "\n",
    "To keep a specific shape preprocessing automatically enables padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598cb3a-e62c-4ef7-a0f4-9ec1fe0d5ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from ultralytics.yolo.utils import ops\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def letterbox(img: np.ndarray, new_shape:Tuple[int, int] = (640, 640), color:Tuple[int, int, int] = (114, 114, 114), auto:bool = False, scale_fill:bool = False, scaleup:bool = False, stride:int = 32):\n",
    "    \"\"\"\n",
    "    Resize image and padding for detection. Takes image as input, \n",
    "    resizes image to fit into new shape with saving original aspect ratio and pads it to meet stride-multiple constraints\n",
    "    \n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "      new_shape (Tuple(int, int)): image size after preprocessing in format [height, width]\n",
    "      color (Tuple(int, int, int)): color for filling padded area\n",
    "      auto (bool): use dynamic input size, only padding for stride constrins applied\n",
    "      scale_fill (bool): scale image to fill new_shape\n",
    "      scaleup (bool): allow scale image if it is lower then desired input size, can affect model accuracy\n",
    "      stride (int): input padding stride\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "      ratio (Tuple(float, float)): hight and width scaling ratio\n",
    "      padding_size (Tuple(int, int)): height and width padding size\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scale_fill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def preprocess_image(img0: np.ndarray):\n",
    "    \"\"\"\n",
    "    Preprocess image according to YOLOv8 input requirements. \n",
    "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
    "    \n",
    "    Parameters:\n",
    "      img0 (np.ndarray): image for preprocessing\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "    \"\"\"\n",
    "    # resize\n",
    "    img = letterbox(img0)[0]\n",
    "    \n",
    "    # Convert HWC to CHW\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def image_to_tensor(image:np.ndarray):\n",
    "    \"\"\"\n",
    "    Preprocess image according to YOLOv8 input requirements. \n",
    "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
    "    \n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "    Returns:\n",
    "      input_tensor (np.ndarray): input tensor in NCHW format with float32 values in [0, 1] range \n",
    "    \"\"\"\n",
    "    input_tensor = image.astype(np.float32)  # uint8 to fp32\n",
    "    input_tensor /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    \n",
    "    # add batch dimension\n",
    "    if input_tensor.ndim == 3:\n",
    "        input_tensor = np.expand_dims(input_tensor, 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87408242-70c2-4774-8e68-fcf79ae0df6e",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "The model output contains detection boxes candidates, it is a tensor with shape `[-1,84,-1]` in format `B,84,N` where:\n",
    "\n",
    "- `B` - batch size\n",
    "- `N` - number of detection boxes\n",
    "\n",
    "Detection box has format [`x`, `y`, `h`, `w`, `class_no_1`, ..., `class_no_80`], where:\n",
    "\n",
    "- (`x`, `y`) - raw coordinates of box center\n",
    "- `h`, `w` - raw height and width of the box\n",
    "- `class_no_1`, ..., `class_no_80` - probability distribution over the classes.\n",
    "\n",
    "For getting the final prediction, we need to apply a non-maximum suppression algorithm and rescale box coordinates to the original image size.\n",
    "\n",
    "The instance segmentation model additionally has an output that contains proto mask candidates for instance segmentation, it should be decoded using box coordinates. It is a tensor with shape [-1 32, -1, -1] in format `B,C H,W` where:\n",
    "- `B` - batch size\n",
    "- `C` - number of candidates\n",
    "- `H` - mask height\n",
    "- `W` - mask width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0c681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess(\n",
    "    pred_boxes:np.ndarray, \n",
    "    input_hw:Tuple[int, int], \n",
    "    orig_img:np.ndarray, \n",
    "    min_conf_threshold:float = 0.25, \n",
    "    nms_iou_threshold:float = 0.7, \n",
    "    agnosting_nms:bool = False, \n",
    "    max_detections:int = 300,\n",
    "    pred_masks:np.ndarray = None,\n",
    "    retina_mask:bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    YOLOv8 model postprocessing function. Applied non maximum supression algorithm to detections and rescale boxes to original image size\n",
    "    Parameters:\n",
    "        pred_boxes (np.ndarray): model output prediction boxes\n",
    "        input_hw (np.ndarray): preprocessed image\n",
    "        orig_image (np.ndarray): image before preprocessing\n",
    "        min_conf_threshold (float, *optional*, 0.25): minimal accepted confidence for object filtering\n",
    "        nms_iou_threshold (float, *optional*, 0.45): minimal overlap score for removing objects duplicates in NMS\n",
    "        agnostic_nms (bool, *optiona*, False): apply class agnostinc NMS approach or not\n",
    "        max_detections (int, *optional*, 300):  maximum detections after NMS\n",
    "        pred_masks (np.ndarray, *optional*, None): model ooutput prediction masks, if not provided only boxes will be postprocessed\n",
    "        retina_mask (bool, *optional*, False): retina mask postprocessing instead of native decoding\n",
    "    Returns:\n",
    "       pred (List[Dict[str, np.ndarray]]): list of dictionary with det - detected boxes in format [x1, y1, x2, y2, score, label] and segment - segmentation polygons for each element in batch\n",
    "    \"\"\"\n",
    "    nms_kwargs = {\"agnostic\": agnosting_nms, \"max_det\":max_detections}\n",
    "    # if pred_masks is not None:\n",
    "    #     nms_kwargs[\"nm\"] = 32\n",
    "    preds = ops.non_max_suppression(\n",
    "        torch.from_numpy(pred_boxes),\n",
    "        min_conf_threshold,\n",
    "        nms_iou_threshold,\n",
    "        nc=80,\n",
    "        **nms_kwargs\n",
    "    )\n",
    "    results = []\n",
    "    proto = torch.from_numpy(pred_masks) if pred_masks is not None else None\n",
    "\n",
    "    for i, pred in enumerate(preds):\n",
    "        shape = orig_img[i].shape if isinstance(orig_img, list) else orig_img.shape\n",
    "        if not len(pred):\n",
    "            results.append({\"det\": [], \"segment\": []})\n",
    "            continue\n",
    "        if proto is None:\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            results.append({\"det\": pred})\n",
    "            continue\n",
    "        if retina_mask:\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            masks = ops.process_mask_native(proto[i], pred[:, 6:], pred[:, :4], shape[:2])  # HWC\n",
    "            segments = [ops.scale_segments(input_hw, x, shape, normalize=False) for x in ops.masks2segments(masks)]\n",
    "        else:\n",
    "            masks = ops.process_mask(proto[i], pred[:, 6:], pred[:, :4], input_hw, upsample=True)\n",
    "            pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "            segments = [ops.scale_segments(input_hw, x, shape, normalize=False) for x in ops.masks2segments(masks)]\n",
    "        results.append({\"det\": pred[:, :6].numpy(), \"segment\": segments})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd163eab-e803-4ae8-96da-22c3bf303630",
   "metadata": {},
   "source": [
    "### Test on single image\n",
    "\n",
    "Now, when we define preprocessing and postprocessing steps, we are ready to check model prediction.\n",
    "\n",
    "First, object detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b41c0-02d4-4357-90a1-9b727f0fcf39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openvino.runtime import Core, Model\n",
    "\n",
    "core = Core()\n",
    "det_ov_model = core.read_model(det_model_path)\n",
    "device = \"CPU\"  # \"GPU\"\n",
    "if device != \"CPU\":\n",
    "    det_ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "det_compiled_model = core.compile_model(det_ov_model, device)\n",
    "\n",
    "\n",
    "def detect(image:np.ndarray, model:Model):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model inference function. Preprocess image, runs model inference and postprocess results using NMS.\n",
    "    Parameters:\n",
    "        image (np.ndarray): input image.\n",
    "        model (Model): OpenVINO compiled model.\n",
    "    Returns:\n",
    "        detections (np.ndarray): detected boxes in format [x1, y1, x2, y2, score, label]\n",
    "    \"\"\"\n",
    "    num_outputs = len(model.outputs)\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    input_tensor = image_to_tensor(preprocessed_image)\n",
    "    result = model(input_tensor)\n",
    "    boxes = result[model.output(0)]\n",
    "    masks = None\n",
    "    if num_outputs > 1:\n",
    "        masks = result[model.output(1)]\n",
    "    input_hw = input_tensor.shape[2:]\n",
    "    detections = postprocess(pred_boxes=boxes, input_hw=input_hw, orig_img=image, pred_masks=masks)\n",
    "    return detections\n",
    "\n",
    "\n",
    "input_image = np.array(Image.open(IMAGE_PATH))\n",
    "detections = detect(input_image, det_compiled_model)[0]\n",
    "image_with_boxes = draw_results(detections, input_image, label_map)\n",
    "\n",
    "Image.fromarray(image_with_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b4862-c182-4ce4-a473-9f38d98deab8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Live demo\n",
    "\n",
    "the following code run model inference on video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bb92e-e301-45a9-b5ff-f7953fad298f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import time\n",
    "from IPython import display\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "\n",
    "\n",
    "# Main processing function to run object detection.\n",
    "def run_object_detection(source=0, flip=False, use_popup=False, skip_first_frames=0, model=det_model, device=device):\n",
    "    player = None\n",
    "    if device != \"CPU\":\n",
    "        model.reshape({0: [1, 3, 640, 640]})\n",
    "    compiled_model = core.compile_model(model, device)\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = utils.VideoPlayer(\n",
    "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
    "        )\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            # Get the results.\n",
    "            input_image = np.array(frame)\n",
    "           \n",
    "            start_time = time.time()\n",
    "            # model expects RGB image, while video capturing in BGR\n",
    "            detections = detect(input_image[:, :, ::-1], compiled_model)[0]\n",
    "            stop_time = time.time()\n",
    "            \n",
    "            image_with_boxes = draw_results(detections, input_image, label_map)\n",
    "            frame = image_with_boxes\n",
    "           \n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "                )\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b4ba6-f478-4417-b09d-93fee5adca41",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "#### Run Live Object Detection and Segmentation\n",
    "\n",
    "Use a webcam as the video input. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`.\n",
    "\n",
    "> NOTE: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server (for example, Binder), the webcam will not work. Popup mode may not work if you run this notebook on a remote computer (for example, Binder).\n",
    "\n",
    "Run the object detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824c15e-5302-43d6-ad61-66205f51abc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_file=\"../data/video/people.mp4\"\n",
    "run_object_detection(source=video_file, flip=True, use_popup=False, model=det_ov_model, device=\"AUTO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
